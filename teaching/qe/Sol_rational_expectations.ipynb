{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "hide-output": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We’ll use the LQ class from quantecon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "hide-output": false
      },
      "outputs": [],
      "source": [
        "from quantecon import LQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1\n",
        "\n",
        "To map a problem into a [discounted optimal linear control\n",
        "problem](lqcontrol.ipynb), we need to define\n",
        "\n",
        "- state vector $ x_t $ and control vector $ u_t $  \n",
        "- matrices $ A, B, Q, R $ that define preferences and the law of\n",
        "  motion for the state  \n",
        "\n",
        "\n",
        "For the state and control vectors we choose\n",
        "\n",
        "$$\n",
        "x_t = \\begin{bmatrix} y_t \\\\ Y_t \\\\ 1 \\end{bmatrix},\n",
        "\\qquad\n",
        "u_t = y_{t+1} - y_{t}\n",
        "$$\n",
        "\n",
        "For $ B, Q, R $ we set\n",
        "\n",
        "$$\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "    1 & 0 & 0 \\\\\n",
        "    0 & \\kappa_1 & \\kappa_0 \\\\\n",
        "    0 & 0 & 1\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "B = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} ,\n",
        "\\quad\n",
        "R =\n",
        "\\begin{bmatrix}\n",
        "    0 & a_1/2 & -a_0/2 \\\\\n",
        "    a_1/2 & 0 & 0 \\\\\n",
        "    -a_0/2 & 0 & 0\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "Q = \\gamma / 2\n",
        "$$\n",
        "\n",
        "By multiplying out you can confirm that\n",
        "\n",
        "- $ x_t' R x_t + u_t' Q u_t = - r_t $  \n",
        "- $ x_{t+1} = A x_t + B u_t $  \n",
        "\n",
        "\n",
        "We’ll use the module `lqcontrol.py` to solve the firm’s problem at the\n",
        "stated parameter values\n",
        "\n",
        "This will return an LQ policy $ F $ with the interpretation\n",
        "$ u_t = - F x_t $, or\n",
        "\n",
        "$$\n",
        "y_{t+1} - y_t = - F_0 y_t - F_1 Y_t - F_2\n",
        "$$\n",
        "\n",
        "Matching parameters with $ y_{t+1} = h_0 + h_1 y_t + h_2 Y_t $ leads\n",
        "to\n",
        "\n",
        "$$\n",
        "h_0 = -F_2, \\quad h_1 = 1 - F_0, \\quad h_2 = -F_1\n",
        "$$\n",
        "\n",
        "Here’s our solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "hide-output": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F = [-0.000, 0.046, -96.949]\n",
            "(h0, h1, h2) = (96.949, 1.000, -0.046)\n"
          ]
        }
      ],
      "source": [
        "# == Model parameters == #\n",
        "\n",
        "a0 = 100\n",
        "a1 = 0.05\n",
        "β = 0.95\n",
        "γ = 10.0\n",
        "\n",
        "# == Beliefs == #\n",
        "\n",
        "κ0 = 95.5\n",
        "κ1 = 0.95\n",
        "\n",
        "# == Formulate the LQ problem == #\n",
        "\n",
        "A = np.array([[1, 0, 0], [0, κ1, κ0], [0, 0, 1]])\n",
        "B = np.array([1, 0, 0])\n",
        "B.shape = 3, 1\n",
        "R = np.array([[0, a1/2, -a0/2], [a1/2, 0, 0], [-a0/2, 0, 0]])\n",
        "Q = 0.5 * γ\n",
        "\n",
        "# == Solve for the optimal policy == #\n",
        "\n",
        "lq = LQ(Q, R, A, B, beta=β)\n",
        "P, F, d = lq.stationary_values()\n",
        "F = F.flatten()\n",
        "out1 = f\"F = [{F[0]:.3f}, {F[1]:.3f}, {F[2]:.3f}]\"\n",
        "h0, h1, h2 = -F[2], 1 - F[0], -F[1]\n",
        "out2 = f\"(h0, h1, h2) = ({h0:.3f}, {h1:.3f}, {h2:.3f})\"\n",
        "\n",
        "print(out1)\n",
        "print(out2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The implication is that\n",
        "\n",
        "$$\n",
        "y_{t+1} = 96.949 + y_t - 0.046 \\, Y_t\n",
        "$$\n",
        "\n",
        "For the case $ n > 1 $, recall that $ Y_t = n y_t $, which,\n",
        "combined with the previous equation, yields\n",
        "\n",
        "$$\n",
        "Y_{t+1}\n",
        "= n \\left( 96.949 + y_t - 0.046 \\, Y_t \\right)\n",
        "= n 96.949 + (1 - n 0.046) Y_t\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2\n",
        "\n",
        "To determine whether a $ \\kappa_0, \\kappa_1 $ pair forms the\n",
        "aggregate law of motion component of a rational expectations\n",
        "equilibrium, we can proceed as follows:\n",
        "\n",
        "- Determine the corresponding firm law of motion\n",
        "  $ y_{t+1} = h_0 + h_1 y_t + h_2 Y_t $  \n",
        "- Test whether the associated aggregate law\n",
        "  :$ Y_{t+1} = n h(Y_t/n, Y_t) $ evaluates to\n",
        "  $ Y_{t+1} = \\kappa_0 + \\kappa_1 Y_t $  \n",
        "\n",
        "\n",
        "In the second step we can use $ Y_t = n y_t = y_t $, so that\n",
        "$ Y_{t+1} = n h(Y_t/n, Y_t) $ becomes\n",
        "\n",
        "$$\n",
        "Y_{t+1} = h(Y_t, Y_t) = h_0 + (h_1 + h_2) Y_t\n",
        "$$\n",
        "\n",
        "Hence to test the second step we can test $ \\kappa_0 = h_0 $ and\n",
        "$ \\kappa_1 = h_1 + h_2 $\n",
        "\n",
        "The following code implements this test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "hide-output": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Equilibrium pair = 95.0818452486, 0.952459076301\n",
            "f(h0, h1, h2) = {h0}, {h1}, {h2}\n"
          ]
        }
      ],
      "source": [
        "candidates = ((94.0886298678, 0.923409232937),\n",
        "              (93.2119845412, 0.984323478873),\n",
        "              (95.0818452486, 0.952459076301))\n",
        "\n",
        "for κ0, κ1 in candidates:\n",
        "\n",
        "    # == Form the associated law of motion == #\n",
        "    A = np.array([[1, 0, 0], [0, κ1, κ0], [0, 0, 1]])\n",
        "\n",
        "    # == Solve the LQ problem for the firm == #\n",
        "    lq = LQ(Q, R, A, B, beta=β)\n",
        "    P, F, d = lq.stationary_values()\n",
        "    F = F.flatten()\n",
        "    h0, h1, h2 = -F[2], 1 - F[0], -F[1]\n",
        "\n",
        "    # == Test the equilibrium condition == #\n",
        "    if np.allclose((κ0, κ1), (h0, h1 + h2)):\n",
        "        print(f'Equilibrium pair = {κ0}, {κ1}')\n",
        "        print('f(h0, h1, h2) = {h0}, {h1}, {h2}')\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output tells us that the answer is pair (iii), which implies\n",
        "$ (h_0, h_1, h_2) = (95.0819, 1.0000, -.0475) $\n",
        "\n",
        "(Notice we use `np.allclose` to test equality of floating point\n",
        "numbers, since exact equality is too strict)\n",
        "\n",
        "Regarding the iterative algorithm, one could loop from a given\n",
        "$ (\\kappa_0, \\kappa_1) $ pair to the associated firm law and then to\n",
        "a new $ (\\kappa_0, \\kappa_1) $ pair\n",
        "\n",
        "This amounts to implementing the operator $ \\Phi $ described in the\n",
        "lecture\n",
        "\n",
        "(There is in general no guarantee that this iterative process will\n",
        "converge to a rational expectations equilibrium)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3\n",
        "\n",
        "We are asked to write the planner problem as an LQ problem\n",
        "\n",
        "For the state and control vectors we choose\n",
        "\n",
        "$$\n",
        "x_t = \\begin{bmatrix} Y_t \\\\ 1 \\end{bmatrix},\n",
        "\\quad\n",
        "u_t = Y_{t+1} - Y_{t}\n",
        "$$\n",
        "\n",
        "For the LQ matrices we set\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix},\n",
        "\\quad\n",
        "B = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix},\n",
        "\\quad\n",
        "R = \\begin{bmatrix} a_1/2 & -a_0/2 \\\\ -a_0/2 & 0 \\end{bmatrix},\n",
        "\\quad\n",
        "Q = \\gamma / 2\n",
        "$$\n",
        "\n",
        "By multiplying out you can confirm that\n",
        "\n",
        "- $ x_t' R x_t + u_t' Q u_t = - s(Y_t, Y_{t+1}) $  \n",
        "- $ x_{t+1} = A x_t + B u_t $  \n",
        "\n",
        "\n",
        "By obtaining the optimal policy and using $ u_t = - F x_t $ or\n",
        "\n",
        "$$\n",
        "Y_{t+1} - Y_t = -F_0 Y_t - F_1\n",
        "$$\n",
        "\n",
        "we can obtain the implied aggregate law of motion via\n",
        "$ \\kappa_0 = -F_1 $ and $ \\kappa_1 = 1-F_0 $\n",
        "\n",
        "The Python code to solve this problem is below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "hide-output": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "95.08187459215002 0.9524590627039248\n"
          ]
        }
      ],
      "source": [
        "# == Formulate the planner's LQ problem == #\n",
        "\n",
        "A = np.array([[1, 0], [0, 1]])\n",
        "B = np.array([[1], [0]])\n",
        "R = np.array([[a1 / 2, -a0 / 2], [-a0 / 2, 0]])\n",
        "Q = γ / 2\n",
        "\n",
        "# == Solve for the optimal policy == #\n",
        "\n",
        "lq = LQ(Q, R, A, B, beta=β)\n",
        "P, F, d = lq.stationary_values()\n",
        "\n",
        "# == Print the results == #\n",
        "\n",
        "F = F.flatten()\n",
        "κ0, κ1 = -F[1], 1 - F[0]\n",
        "print(κ0, κ1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output yields the same $ (\\kappa_0, \\kappa_1) $ pair obtained as\n",
        "an equilibrium from the previous exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4\n",
        "\n",
        "The monopolist’s LQ problem is almost identical to the planner’s problem\n",
        "from the previous exercise, except that\n",
        "\n",
        "$$\n",
        "R = \\begin{bmatrix}\n",
        "    a_1 & -a_0/2 \\\\\n",
        "    -a_0/2 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The problem can be solved as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "hide-output": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "73.47294403502818 0.9265270559649701\n"
          ]
        }
      ],
      "source": [
        "A = np.array([[1, 0], [0, 1]])\n",
        "B = np.array([[1], [0]])\n",
        "R = np.array([[a1, -a0 / 2], [-a0 / 2, 0]])\n",
        "Q = γ / 2\n",
        "\n",
        "lq = LQ(Q, R, A, B, beta=β)\n",
        "P, F, d = lq.stationary_values()\n",
        "\n",
        "F = F.flatten()\n",
        "m0, m1 = -F[1], 1 - F[0]\n",
        "print(m0, m1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the law of motion for the monopolist is approximately\n",
        "$ Y_{t+1} = 73.4729 + 0.9265 Y_t $\n",
        "\n",
        "In the rational expectations case the law of motion was approximately\n",
        "$ Y_{t+1} = 95.0818 + 0.9525 Y_t $\n",
        "\n",
        "One way to compare these two laws of motion is by their fixed points,\n",
        "which give long run equilibrium output in each case\n",
        "\n",
        "For laws of the form $ Y_{t+1} = c_0 + c_1 Y_t $, the fixed point is\n",
        "$ c_0 / (1 - c_1) $\n",
        "\n",
        "If you crunch the numbers, you will see that the monopolist adopts a\n",
        "lower long run quantity than obtained by the competitive market,\n",
        "implying a higher market price\n",
        "\n",
        "This is analogous to the elementary static-case results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Footnotes**\n",
        "\n",
        "<p><a id=fn-im href=#fn-im-link><strong>[1]</strong></a> A literature that studies whether models populated  with agents\n",
        "who learn can converge  to rational expectations equilibria features\n",
        "iterations on a modification of the mapping $ \\Phi $ that can be\n",
        "approximated as $ \\gamma \\Phi + (1-\\gamma)I $. Here $ I $ is the\n",
        "identity operator and $ \\gamma \\in (0,1) $ is a *relaxation parameter*.\n",
        "See [[MS89]](zreferences.html#marcetsargent1989) and [[EH01]](https://lectures.quantecon.org/py/zreferences.ipynb#evanshonkapohja2001) for statements\n",
        "and applications of this approach to establish conditions under which\n",
        "collections of adaptive agents who use least squares learning converge to a\n",
        "rational expectations equilibrium."
      ]
    }
  ],
  "metadata": {
    "filename": "rational_expectations.rst",
    "kernelspec": {
      "display_name": "Python",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "title": "Rational Expectations Equilibrium"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}